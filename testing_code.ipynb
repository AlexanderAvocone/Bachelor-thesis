{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([112161, 147268,  94416, 148257,  94722, 140472, 317818, 277114,\n",
      "             81255, 139804,\n",
      "            ...\n",
      "            123854, 410955, 379499, 357911, 377987, 206924, 347132,  74078,\n",
      "            330749,  59837],\n",
      "           dtype='int64', length=141856)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/aavocone/anaconda3/envs/Bachelor/lib/python3.8/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/work/aavocone/Code/testing_code.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bportal1/work/aavocone/Code/testing_code.ipynb#ch0000001vscode-remote?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(ytest\u001b[39m.\u001b[39mindex)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bportal1/work/aavocone/Code/testing_code.ipynb#ch0000001vscode-remote?line=19'>20</a>\u001b[0m model \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39mXGBClassifier(n_estimators\u001b[39m=\u001b[39m \u001b[39m600\u001b[39m, verbosity \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, use_label_encoder\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bportal1/work/aavocone/Code/testing_code.ipynb#ch0000001vscode-remote?line=20'>21</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(xtrain, ytrain)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bportal1/work/aavocone/Code/testing_code.ipynb#ch0000001vscode-remote?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mdone\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/work/aavocone/anaconda3/envs/Bachelor/lib/python3.8/site-packages/xgboost/core.py:506\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    505\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 506\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/work/aavocone/anaconda3/envs/Bachelor/lib/python3.8/site-packages/xgboost/sklearn.py:1250\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1230\u001b[0m model, feval, params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_configure_fit(xgb_model, eval_metric, params)\n\u001b[1;32m   1231\u001b[0m train_dmatrix, evals \u001b[39m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1232\u001b[0m     missing\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmissing,\n\u001b[1;32m   1233\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1247\u001b[0m     label_transform\u001b[39m=\u001b[39mlabel_transform,\n\u001b[1;32m   1248\u001b[0m )\n\u001b[0;32m-> 1250\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[1;32m   1251\u001b[0m     params,\n\u001b[1;32m   1252\u001b[0m     train_dmatrix,\n\u001b[1;32m   1253\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[1;32m   1254\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[1;32m   1255\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[1;32m   1256\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[1;32m   1257\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[1;32m   1258\u001b[0m     feval\u001b[39m=\u001b[39;49mfeval,\n\u001b[1;32m   1259\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   1260\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1261\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   1262\u001b[0m )\n\u001b[1;32m   1264\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n\u001b[1;32m   1265\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective \u001b[39m=\u001b[39m params[\u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/work/aavocone/anaconda3/envs/Bachelor/lib/python3.8/site-packages/xgboost/training.py:188\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(params, dtrain, num_boost_round\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, evals\u001b[39m=\u001b[39m(), obj\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, feval\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    116\u001b[0m           maximize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, early_stopping_rounds\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, evals_result\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    117\u001b[0m           verbose_eval\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, xgb_model\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, callbacks\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    118\u001b[0m     \u001b[39m# pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[39m\"\"\"Train a booster with given parameters.\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \n\u001b[1;32m    121\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[39m    Booster : a trained booster model\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m     bst \u001b[39m=\u001b[39m _train_internal(params, dtrain,\n\u001b[1;32m    189\u001b[0m                           num_boost_round\u001b[39m=\u001b[39;49mnum_boost_round,\n\u001b[1;32m    190\u001b[0m                           evals\u001b[39m=\u001b[39;49mevals,\n\u001b[1;32m    191\u001b[0m                           obj\u001b[39m=\u001b[39;49mobj, feval\u001b[39m=\u001b[39;49mfeval,\n\u001b[1;32m    192\u001b[0m                           xgb_model\u001b[39m=\u001b[39;49mxgb_model, callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    193\u001b[0m                           verbose_eval\u001b[39m=\u001b[39;49mverbose_eval,\n\u001b[1;32m    194\u001b[0m                           evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[1;32m    195\u001b[0m                           maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    196\u001b[0m                           early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds)\n\u001b[1;32m    197\u001b[0m     \u001b[39mreturn\u001b[39;00m bst\n",
      "File \u001b[0;32m/work/aavocone/anaconda3/envs/Bachelor/lib/python3.8/site-packages/xgboost/training.py:81\u001b[0m, in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39mif\u001b[39;00m callbacks\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m     80\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m bst\u001b[39m.\u001b[39;49mupdate(dtrain, i, obj)\n\u001b[1;32m     82\u001b[0m \u001b[39mif\u001b[39;00m callbacks\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m     83\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/work/aavocone/anaconda3/envs/Bachelor/lib/python3.8/site-packages/xgboost/core.py:1680\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_features(dtrain)\n\u001b[1;32m   1679\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1680\u001b[0m     _check_call(_LIB\u001b[39m.\u001b[39;49mXGBoosterUpdateOneIter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[1;32m   1681\u001b[0m                                             ctypes\u001b[39m.\u001b[39;49mc_int(iteration),\n\u001b[1;32m   1682\u001b[0m                                             dtrain\u001b[39m.\u001b[39;49mhandle))\n\u001b[1;32m   1683\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1684\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import functions as fc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"/work/aavocone/small_set.csv\")\n",
    "\n",
    "#test train split\n",
    "X = df[df.columns[:-3]]    #exclude \"signal\" \"classification\" \"B_sig_isSignalAcceptMissingNeutrino\"\n",
    "y = df[\"signal\"]            \n",
    "indicies = df[\"class\"]\n",
    "\n",
    "xtrain,xtest,ytrain,ytest= train_test_split(X, y, test_size = 0.33, stratify = y)\n",
    "\n",
    "xtrain,xval,ytrain,yval= train_test_split(xtrain, ytrain,  test_size = 0.5, stratify= ytrain)\n",
    "print(ytest.index)\n",
    "\n",
    "model = xgb.XGBClassifier(n_estimators= 600, verbosity = 0, use_label_encoder=False)\n",
    "model.fit(xtrain, ytrain)\n",
    "print(\"done\")\n",
    "\n",
    "\n",
    "#to long 3m\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('Bachelor': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8fa6b6c428d539fc06232ec94d1d357953a237439554a85c30960535ca4a6246"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
